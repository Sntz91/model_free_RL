{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "USE_PIL = True\n",
    "if USE_PIL:\n",
    "    # you should use pillow-simd, as it is faster than stardand Pillow\n",
    "    from PIL import Image\n",
    "else:\n",
    "    import cv2\n",
    "    cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Change image shape to CWH\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        new_shape = (old_shape[-1], old_shape[0], old_shape[1])\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0.0, high=1.0, shape=new_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.swapaxes(observation, 2, 0)\n",
    "    \n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # careful! This undoes the memory optimization, use\n",
    "        # with smaller replay buffers only.\n",
    "        return np.array(observation).astype(np.float32) / 255.0\n",
    "    \n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env, width=84, height=84, grayscale=True, dict_space_key=None):\n",
    "        \"\"\"\n",
    "        Warp frames to 84x84 as done in the Nature paper and later work.\n",
    "        If the environment uses dictionary observations, `dict_space_key` can be specified which indicates which\n",
    "        observation should be warped.\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self._width = width\n",
    "        self._height = height\n",
    "        self._grayscale = grayscale\n",
    "        self._key = dict_space_key\n",
    "        if self._grayscale:\n",
    "            num_colors = 1\n",
    "        else:\n",
    "            num_colors = 3\n",
    "\n",
    "        new_space = gym.spaces.Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(self._height, self._width, num_colors),\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "        if self._key is None:\n",
    "            original_space = self.observation_space\n",
    "            self.observation_space = new_space\n",
    "        else:\n",
    "            original_space = self.observation_space.spaces[self._key]\n",
    "            self.observation_space.spaces[self._key] = new_space\n",
    "        assert original_space.dtype == np.uint8 and len(original_space.shape) == 3\n",
    "\n",
    "    def observation(self, obs):\n",
    "        if self._key is None:\n",
    "            frame = obs\n",
    "        else:\n",
    "            frame = obs[self._key]\n",
    "        if USE_PIL:\n",
    "            frame = Image.fromarray(frame)\n",
    "            if self._grayscale:\n",
    "                frame = frame.convert(\"L\")\n",
    "            frame = frame.resize((self._width, self._height))\n",
    "            frame = np.array(frame)\n",
    "        else:\n",
    "            if self._grayscale:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "            frame = cv2.resize(\n",
    "                frame, (self._width, self._height),\n",
    "                interpolation=cv2.INTER_AREA\n",
    "            )\n",
    "        if self._grayscale:\n",
    "            frame = np.expand_dims(frame, -1)\n",
    "\n",
    "        if self._key is None:\n",
    "            obs = frame\n",
    "        else:\n",
    "            obs = obs.copy()\n",
    "            obs[self._key] = frame\n",
    "        return obs\n",
    "\n",
    "env_name = 'SeaquestNoFrameskip-v4'\n",
    "env = gym.make(env_name)\n",
    "env = WarpFrame(env)\n",
    "env = ImageToPyTorch(env)\n",
    "#env = ScaledFloatFrame(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class DRQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Network, choosing actions\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(DRQN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(n_in[0], 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        conv_out_size = self._get_conv_out(n_in)\n",
    "        self.lstm = nn.LSTM(input_size=conv_out_size, hidden_size=512)\n",
    "        self.linear = nn.Linear(512, 18)\n",
    "        \n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.activation3 = nn.ReLU()\n",
    "        self.activation4 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "\n",
    "        self.dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor #here\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv1(torch.zeros(1, *shape))\n",
    "        o = self.conv2(o)\n",
    "        o = self.conv3(o)\n",
    "        return int(np.prod(o.size())) #3136 (int)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = x.type(self.dtype)\n",
    "        #for the conv just make batch_size * sequence and then split \n",
    "        sequence_length = x.shape[0]\n",
    "        batch_size = x.shape[1]\n",
    "        x = x.reshape(sequence_length * batch_size, 1, 84, 84)\n",
    "        \n",
    "        conv1_out = self.activation1(self.conv1(x))\n",
    "        conv2_out = self.activation2(self.conv2(conv1_out))\n",
    "        conv3_out = self.activation3(self.conv3(conv2_out))\n",
    "        conv3_out_flattened = conv3_out.view(conv3_out.size()[0], -1)\n",
    "        conv3_out_flattened = conv3_out_flattened.reshape(sequence_length, batch_size, 3136)\n",
    "        \n",
    "        lstm_output, lstm_hidden = self.lstm(conv3_out_flattened, hidden)\n",
    "        lstm_output = self.activation4(lstm_output)\n",
    "        output = self.linear(lstm_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new episode\n",
      "new episode\n",
      "new episode\n",
      "new episode\n",
      "new episode\n",
      "new episode\n",
      "new episode\n",
      "new episode\n",
      "new episode\n",
      "new episode\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, List, Union\n",
    "from collections import namedtuple, deque\n",
    "import torch\n",
    "\n",
    "Experience = namedtuple(\"Experience\",\n",
    "                           field_names = [\"observation\", \"action\", \"reward\"])\n",
    "\n",
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "    Replay Memory for Recurrent Neural Networks\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity: int, n_sequence_length: int):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.n_sequence_length = n_sequence_length\n",
    "  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    \n",
    "    def append(self, exp_sequence):\n",
    "        #if episode not long enough (n_sequence_length), then reject it\n",
    "        if(len(exp_sequence) > self.n_sequence_length):\n",
    "            self.buffer.append(exp_sequence)\n",
    "        \n",
    "        \n",
    "    def sample_episode(self):\n",
    "        idx = np.random.choice(len(self))\n",
    "        return self.buffer[idx]\n",
    "        \n",
    "        \n",
    "    def sample(self, batch_size: int = 1) -> Tuple:\n",
    "        #So a batch now contains batch_size *  sequences, right?\n",
    "        \n",
    "        #trajectories = []\n",
    "        obs = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        for _ in range(batch_size):\n",
    "            episode = self.sample_episode()\n",
    "\n",
    "            start_idx = np.random.choice(len(episode)-self.n_sequence_length)\n",
    "            trajectory = episode[start_idx : (start_idx + self.n_sequence_length)]\n",
    "            obs_, actions_, rewards_ = zip(*[[t.observation, t.action, t.reward] for t in trajectory])\n",
    "            \n",
    "            actions.append(actions_)\n",
    "            rewards.append(rewards_)\n",
    "            obs.append(obs_)\n",
    "            \n",
    "            #trajectories.append(trajectory)\n",
    "        \n",
    "        return [obs, actions, rewards]\n",
    "    \n",
    "    def sample_(self, batch_size: int = 1) -> Tuple:\n",
    "               #So a batch now contains batch_size *  sequences, right?\n",
    "        \n",
    "        trajectories = []\n",
    "        for _ in range(batch_size):\n",
    "            episode = self.sample_episode()\n",
    "\n",
    "            start_idx = np.random.choice(len(episode)-self.n_sequence_length)\n",
    "            trajectory = episode[start_idx : (start_idx + self.n_sequence_length)]\n",
    "            trajectories.append(trajectory)\n",
    "        \n",
    "        return [obs, actions, rewards]\n",
    "\n",
    "    \n",
    "    \n",
    "rm = ReplayMemory(10, 5)\n",
    "for i in range(10):\n",
    "    print('new episode')\n",
    "    o = env.reset()\n",
    "    done = False\n",
    "    i = 0\n",
    "    trajectory = []\n",
    "    while(not done):\n",
    "        i+=1\n",
    "        action = np.random.choice(env.action_space.n)\n",
    "        o_next, reward, done, _ = env.step(action)\n",
    "        exp = Experience(o, action, reward)\n",
    "        trajectory.append(exp)\n",
    "        o = o_next\n",
    "    rm.append(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 1, 84, 84])"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "obs, actions, rewards = rm.sample(BATCH_SIZE)\n",
    "obs = torch.tensor(obs).swapaxes(1, 0).to(device)\n",
    "actions = torch.tensor(actions).swapaxes(1, 0).to(device)\n",
    "rewards = torch.tensor(rewards).swapaxes(1, 0).to(device)\n",
    "#m,b,/a/r\n",
    "\n",
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18])"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = DRQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "hidden = (torch.randn(1, BATCH_SIZE, 512).to(device), torch.randn(1, BATCH_SIZE, 512).to(device))\n",
    "\n",
    "out = network(obs, hidden)\n",
    "out[-1][0].shape\n",
    "\n",
    "#now only backprop some errors after burn-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DRQN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (lstm): LSTM(3136, 512)\n",
       ")"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
