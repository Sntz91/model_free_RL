{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Store observations $o \\in \\Omega$ instead of states ✓\n",
    "* Store recurrent state in replay to initialize the network at training time (optional)\n",
    "* Instead of storing $(s, a, r, s')$ tuples, store sequences of $(s, a, r)$ with fixed-length (m=80). ✓ \n",
    "    * Adjacent sequences should overlap by $l=40$, because this is used for burn-in.  ✓\n",
    "    * Never cross episode Boundries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, List, Union\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "Experience = namedtuple(\"Experience\",\n",
    "                           field_names = [\"observation\", \"action\", \"reward\"])\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity: int, n_burn_in: int, n_sequence_length: int):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.n_burn_in = n_burn_in\n",
    "        self.n_sequence_length = n_sequence_length\n",
    "  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    \n",
    "    def append(self, exp_sequence):\n",
    "        #length of sequence should be l\n",
    "        assert(len(exp_sequence) == self.n_sequence_length)\n",
    "        \n",
    "        #sequences should overlap by m\n",
    "        assert(1==1)\n",
    "        \n",
    "        self.buffer.append(exp_sequence)\n",
    "        \n",
    "        \n",
    "    def sample(self, batch_size: int = 1) -> Tuple:\n",
    "        idxs = np.random.choice(len(self), batch_size, replace=False)\n",
    "        \n",
    "        states, actions, rewards, dones, next_states = \\\n",
    "            zip(*[self.buffer[idx] for idx in idxs])\n",
    "        \n",
    "        return np.array(states), np.array(actions), \\\n",
    "            np.array(rewards, dtype=np.float32), np.array(dones, dtype=bool), \\\n",
    "            np.array(next_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append with window l, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = 0\n",
    "n_burn_in = 3\n",
    "n_sequence_length = 5\n",
    "rm = ReplayMemory(10, n_burn_in, n_sequence_length)\n",
    "exp = []\n",
    "\n",
    "for i in range(1,11):\n",
    "    exp.append([Experience(i, i, i)])\n",
    "    if(i<=n_sequence_length and i%n_sequence_length == 0 and i>0):\n",
    "        #print('first append with i=%d' % i)\n",
    "        rm.append(exp)\n",
    "        exp = exp[-n_burn_in:]\n",
    "        tmp = 0\n",
    "    if(i>n_sequence_length and tmp==n_sequence_length-n_burn_in):\n",
    "        #print('append with i=%d' % i) \n",
    "        rm.append(exp)\n",
    "        exp = exp[-n_burn_in:]\n",
    "        tmp = 0\n",
    "    tmp = tmp+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([[[Experience(observation=1, action=1, reward=1)],\n",
       "        [Experience(observation=2, action=2, reward=2)],\n",
       "        [Experience(observation=3, action=3, reward=3)],\n",
       "        [Experience(observation=4, action=4, reward=4)],\n",
       "        [Experience(observation=5, action=5, reward=5)]],\n",
       "       [[Experience(observation=3, action=3, reward=3)],\n",
       "        [Experience(observation=4, action=4, reward=4)],\n",
       "        [Experience(observation=5, action=5, reward=5)],\n",
       "        [Experience(observation=6, action=6, reward=6)],\n",
       "        [Experience(observation=7, action=7, reward=7)]],\n",
       "       [[Experience(observation=5, action=5, reward=5)],\n",
       "        [Experience(observation=6, action=6, reward=6)],\n",
       "        [Experience(observation=7, action=7, reward=7)],\n",
       "        [Experience(observation=8, action=8, reward=8)],\n",
       "        [Experience(observation=9, action=9, reward=9)]]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good, but never across episode boundries :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good, but now more efficient code! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Although this is how it is described in R2D2, for me that doesnt make any sense, because we would need to store duplicates!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just store $(s,a,r)$ normally and generate sequences, when sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
