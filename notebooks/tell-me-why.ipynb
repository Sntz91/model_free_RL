{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ.setdefault('PATH', '')\n",
    "from collections import deque\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "USE_PIL = True\n",
    "if USE_PIL:\n",
    "    # you should use pillow-simd, as it is faster than stardand Pillow\n",
    "    from PIL import Image\n",
    "else:\n",
    "    import cv2\n",
    "    cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "\n",
    "class TimeLimit(gym.Wrapper):\n",
    "    def __init__(self, env, max_episode_steps=None):\n",
    "        super(TimeLimit, self).__init__(env)\n",
    "        self._max_episode_steps = max_episode_steps\n",
    "        self._elapsed_steps = 0\n",
    "\n",
    "    def step(self, ac):\n",
    "        observation, reward, done, info = self.env.step(ac)\n",
    "        self._elapsed_steps += 1\n",
    "        if self._elapsed_steps >= self._max_episode_steps:\n",
    "            done = True\n",
    "            info['TimeLimit.truncated'] = True\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self._elapsed_steps = 0\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "class ClipActionsWrapper(gym.Wrapper):\n",
    "    def step(self, action):\n",
    "        import numpy as np\n",
    "        action = np.nan_to_num(action)\n",
    "        action = np.clip(action, self.action_space.low, self.action_space.high)\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "\n",
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
    "        No-op is assumed to be action 0.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset(**kwargs)\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
    "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done  = True\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n",
    "            # so it's important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip       = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1: self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Note that the observation on the done=True frame\n",
    "        # doesn't matter\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "class ClipRewardEnv(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
    "        return np.sign(reward)\n",
    "\n",
    "\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env, width=84, height=84, grayscale=True, dict_space_key=None):\n",
    "        \"\"\"\n",
    "        Warp frames to 84x84 as done in the Nature paper and later work.\n",
    "        If the environment uses dictionary observations, `dict_space_key` can be specified which indicates which\n",
    "        observation should be warped.\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self._width = width\n",
    "        self._height = height\n",
    "        self._grayscale = grayscale\n",
    "        self._key = dict_space_key\n",
    "        if self._grayscale:\n",
    "            num_colors = 1\n",
    "        else:\n",
    "            num_colors = 3\n",
    "\n",
    "        new_space = gym.spaces.Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(self._height, self._width, num_colors),\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "        if self._key is None:\n",
    "            original_space = self.observation_space\n",
    "            self.observation_space = new_space\n",
    "        else:\n",
    "            original_space = self.observation_space.spaces[self._key]\n",
    "            self.observation_space.spaces[self._key] = new_space\n",
    "        assert original_space.dtype == np.uint8 and len(original_space.shape) == 3\n",
    "\n",
    "    def observation(self, obs):\n",
    "        if self._key is None:\n",
    "            frame = obs\n",
    "        else:\n",
    "            frame = obs[self._key]\n",
    "        if USE_PIL:\n",
    "            frame = Image.fromarray(frame)\n",
    "            if self._grayscale:\n",
    "                frame = frame.convert(\"L\")\n",
    "            frame = frame.resize((self._width, self._height))\n",
    "            frame = np.array(frame)\n",
    "        else:\n",
    "            if self._grayscale:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "            frame = cv2.resize(\n",
    "                frame, (self._width, self._height),\n",
    "                interpolation=cv2.INTER_AREA\n",
    "            )\n",
    "        if self._grayscale:\n",
    "            frame = np.expand_dims(frame, -1)\n",
    "\n",
    "        if self._key is None:\n",
    "            obs = frame\n",
    "        else:\n",
    "            obs = obs.copy()\n",
    "            obs[self._key] = frame\n",
    "        return obs\n",
    "\n",
    "class LazyFrames(object):\n",
    "    def __init__(self, frames):\n",
    "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
    "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
    "        buffers.\n",
    "        This object should only be converted to numpy array before being passed to the model.\n",
    "        You'd not believe how complex the previous solution was.\"\"\"\n",
    "        self._frames = frames\n",
    "        self._out = None\n",
    "\n",
    "    def _force(self):\n",
    "        if self._out is None:\n",
    "            self._out = np.concatenate(self._frames, axis=0)\n",
    "            self._frames = None\n",
    "        return self._out\n",
    "\n",
    "    def __array__(self, dtype=None):\n",
    "        out = self._force()\n",
    "        if dtype is not None:\n",
    "            out = out.astype(dtype)\n",
    "        return out\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._force())\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self._force()[i]\n",
    "\n",
    "    def count(self):\n",
    "        frames = self._force()\n",
    "        return frames.shape[1:frames.ndim]\n",
    "\n",
    "    def frame(self, i):\n",
    "        return self._force()[i, ...]\n",
    "\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        \"\"\"Stack k last frames.\n",
    "        Returns lazy array, which is much more memory efficient.\n",
    "        See Also\n",
    "        --------\n",
    "        baselines.common.atari_wrappers.LazyFrames\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[0]*k, shp[1], shp[2]), dtype=env.observation_space.dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        ob = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(ob)\n",
    "        return self._get_ob()\n",
    "\n",
    "    def step(self, action):\n",
    "        ob, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(ob)\n",
    "        return self._get_ob(), reward, done, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        assert len(self.frames) == self.k\n",
    "        return LazyFrames(list(self.frames))\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # careful! This undoes the memory optimization, use\n",
    "        # with smaller replay buffers only.\n",
    "        return np.array(observation).astype(np.float32) / 255.0\n",
    "\n",
    "class SkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self._skip       = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "\n",
    "def make_atari(env_id, max_episode_steps=None,\n",
    "               skip_noop=False, skip_maxskip=False):\n",
    "    env = gym.make(env_id)\n",
    "    assert 'NoFrameskip' in env.spec.id\n",
    "    if not skip_noop:\n",
    "        env = NoopResetEnv(env, noop_max=30)\n",
    "    if not skip_maxskip:\n",
    "        env = MaxAndSkipEnv(env, skip=4)\n",
    "    else:\n",
    "        env = SkipEnv(env, skip=4)\n",
    "    if max_episode_steps is not None:\n",
    "        env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "    return env\n",
    "\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Change image shape to CWH\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        new_shape = (old_shape[-1], old_shape[0], old_shape[1])\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0.0, high=1.0, shape=new_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.swapaxes(observation, 2, 0)\n",
    "\n",
    "\n",
    "def wrap_deepmind(env, episode_life=True, clip_rewards=True,\n",
    "                  frame_stack=False, scale=False, pytorch_img=False,\n",
    "                  frame_stack_count=4, skip_firereset=False):\n",
    "    \"\"\"Configure environment for DeepMind-style Atari.\n",
    "    \"\"\"\n",
    "    if episode_life:\n",
    "        env = EpisodicLifeEnv(env)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        if not skip_firereset:\n",
    "            env = FireResetEnv(env)\n",
    "    env = WarpFrame(env)\n",
    "    if pytorch_img:\n",
    "        env = ImageToPyTorch(env)\n",
    "    if scale:\n",
    "        env = ScaledFloatFrame(env)\n",
    "    if clip_rewards:\n",
    "        env = ClipRewardEnv(env)\n",
    "    if frame_stack:\n",
    "        env = FrameStack(env, frame_stack_count)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union\n",
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "\n",
    "Experience = namedtuple(\"Experience\", \n",
    "                            field_names = [\"state\", \"action\", \"reward\", \"done\", \"next_state\"])\n",
    "\n",
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "    Original Replay Memory by Lin. Used for vanilla DQN, no prioritized Replay or bootstrapping with n>1.\n",
    "    Used to store and sample experiences\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            capacity: size of buffer\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, sample: Experience) -> None:\n",
    "        \"\"\"\n",
    "        Append sample\n",
    "        Args:\n",
    "            sample: A sample of an experience to store. Experience is a tuple(state, action, reward, done, next_state)\n",
    "        \"\"\"\n",
    "        self.buffer.append(sample)\n",
    "\n",
    "    def sample(self, batch_size: int = 1) -> Tuple:\n",
    "        \"\"\"\n",
    "        Return batch of buffer, randomly (uniformely).\n",
    "        Args: \n",
    "            batch_size: size of batch\n",
    "        \"\"\"\n",
    "        idxs = np.random.choice(len(self), batch_size, replace=False)\n",
    "\n",
    "        states, actions, rewards, dones, next_states = \\\n",
    "            zip(*[self.buffer[idx] for idx in idxs])\n",
    "\n",
    "        return np.array(states), np.array(actions), \\\n",
    "            np.array(rewards, dtype=np.float32), np.array(dones, dtype=bool), \\\n",
    "            np.array(next_states)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Network, choosing actions\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(n_in[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(n_in)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_out)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.float() #!!! not sure. I need this, because the images are now stored as uint8 and not float32 anymore\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "from datetime import datetime\n",
    "import gym\n",
    "import os\n",
    "\n",
    "\n",
    "class DQN_Agent():\n",
    "    \"\"\"\n",
    "        Vanilla DQN Agent from Mnih2013, Mnih2015\n",
    "    \"\"\"\n",
    "    def __init__(self, env, env_name):\n",
    "        #Hyperparameters\n",
    "        self.hp_replay_memory_capacity = 10000\n",
    "        self.hp_gamma = 0.99\n",
    "        self.hp_epsilon_start = 1.0\n",
    "        self.hp_epsilon_end = 0.1\n",
    "        self.hp_epsilon_decay_last_frame = 100000\n",
    "        self.hp_learning_rate = 0.00015\n",
    "        self.hp_replay_memory_start_after = 10000\n",
    "        self.hp_batch_size = 32\n",
    "        self.hp_target_update_after = 10000\n",
    "        \n",
    "        #Training loop: Episode length differs in each game, so timesteps / frames are better!\n",
    "        self.nr_of_total_frames = 2e6\n",
    "        self.nr_of_evaluation_frames = 10e3\n",
    "        self.nr_of_frames_before_evaluation = 50e3\n",
    "\n",
    "        #Evaluation variables\n",
    "        self.timesteps_overall = -1\n",
    "        self.timesteps_after_last_episode = 0\n",
    "        self.train_obtained_returns = []\n",
    "        self.train_avg_returns = []\n",
    "        self.eval_obtained_returns = []\n",
    "        self.eval_counter = 0\n",
    "        self.eval_epsilon = 0.05\n",
    "        self.eval_state_samples = []\n",
    "\n",
    "        self.env = env\n",
    "        self.env_name = env_name\n",
    "           \n",
    "        self.replay_memory = ReplayMemory(self.hp_replay_memory_capacity)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "        self.training_start_timestamp = datetime.now(tz=None).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "        #Network\n",
    "        self.policy_net = DQN(self.env.observation_space.shape, self.env.action_space.n).to(self.device)\n",
    "        self.target_net = DQN(self.env.observation_space.shape, self.env.action_space.n).to(self.device)\n",
    "        self._update_target_net()\n",
    "\n",
    "\n",
    "\n",
    "    def _update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"\n",
    "            reset environment and reset the obtained return\n",
    "        \"\"\"\n",
    "        self.state = self.env.reset()\n",
    "        self.last_obtained_return = 0.0\n",
    "\n",
    "\n",
    "    def _calc_loss(self, batch):\n",
    "        \"\"\"\n",
    "            Calculate L1-Loss for given batch.\n",
    "        \"\"\"\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "    \n",
    "        states_v = torch.from_numpy(states).to(self.device)\n",
    "        next_states_v = torch.from_numpy(next_states).to(self.device)\n",
    "        actions_v = torch.from_numpy(actions).to(self.device)\n",
    "        rewards_v = torch.from_numpy(rewards).to(self.device)\n",
    "        done_mask = torch.BoolTensor(dones).to(self.device)\n",
    "        \n",
    "        state_action_values = self.policy_net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_state_values = self.target_net(next_states_v).max(1)[0]\n",
    "            next_state_values[done_mask] = 0.0\n",
    "        next_state_values = next_state_values.detach()\n",
    "        expected_state_action_values = (next_state_values * self.hp_gamma) + rewards_v\n",
    "\n",
    "        return F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "\n",
    "\n",
    "    def _get_epsilon(self):\n",
    "        \"\"\"\n",
    "            Get current value for epsilon\n",
    "        \"\"\"\n",
    "        return max(self.hp_epsilon_end, self.hp_epsilon_start - self.timesteps_overall / self.hp_epsilon_decay_last_frame)\n",
    "\n",
    "\n",
    "    def _select_action(self, eval):\n",
    "        \"\"\"\n",
    "            Select action\n",
    "        \"\"\"\n",
    "        epsilon = self._get_epsilon()\n",
    "        if eval:\n",
    "            epsilon = self.eval_epsilon\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            state = torch.from_numpy(np.array([self.state])).type(self.dtype)#.unsqueeze(0) / 255.0\n",
    "            #q_vals = self.policy_net(torch.tensor(np.array([self.state], copy=False)).to(self.device))\n",
    "            q_vals = self.policy_net(state)\n",
    "            _, action = torch.max(q_vals, dim=1)\n",
    "            action = action.item()\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def _play_step(self, eval=False):\n",
    "        \"\"\"\n",
    "            Play one step and return if episode ended\n",
    "        \"\"\"\n",
    "        action = self._select_action(eval)\n",
    "        \n",
    "        next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "        self.last_obtained_return += reward\n",
    "\n",
    "        if not eval:\n",
    "            exp = Experience(self.state, action, reward, done, next_state)\n",
    "            self.replay_memory.append(exp)\n",
    "\n",
    "        self.state = next_state\n",
    "\n",
    "        return done\n",
    "\n",
    "\n",
    "\n",
    "    def _evaluate(self, writer):\n",
    "        \"\"\"\n",
    "            Pause Training and evaluate agent by running environment like training, \n",
    "            but without actually training the model and decreased exploration.\n",
    "        \"\"\"\n",
    "        _eval_iteration_returns = []\n",
    "        _eval_iteration_max_q_vals = []\n",
    "\n",
    "        print(\"***************** Start Evaluation *****************\")\n",
    "        evaluation_episode = 0\n",
    "        evaluation_frames = 0\n",
    "        while True:\n",
    "            evaluation_episode += 1\n",
    "            self._reset()\n",
    "            done = False\n",
    "            while(not done):\n",
    "                evaluation_frames += 1\n",
    "                done = self._play_step(eval=True) \n",
    "\n",
    "            _eval_iteration_returns.append(self.last_obtained_return)\n",
    "            print(\"Evaluation episode %d ended with return %d\" %(evaluation_episode, self.last_obtained_return))      \n",
    "\n",
    "            if evaluation_frames > self.nr_of_evaluation_frames:\n",
    "                break\n",
    "\n",
    "        _score_avg_returns = np.mean(_eval_iteration_returns)\n",
    "        _score_max_q = np.mean(_eval_iteration_max_q_vals)\n",
    "\n",
    "        print(\"Avg evaluation score: %f\" % _score_avg_returns)\n",
    "        print(\"***************** End Evaluation *****************\")\n",
    "\n",
    "        self.eval_counter += 1\n",
    "        self.eval_obtained_returns.append(_score_avg_returns)\n",
    "\n",
    "        writer.add_scalar('Evaluation/AvgTotalReturnPerEpisode', _score_avg_returns, self.eval_counter)\n",
    "        writer.add_scalar('Evaluation/MaxPredQVals', _score_max_q, self.eval_counter)\n",
    "\n",
    "\n",
    "    def train_agent(self):\n",
    "        \"\"\"\n",
    "            Train the Agent. \n",
    "        \"\"\"\n",
    "        training_timestamp = datetime.now(tz=None).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        writer = SummaryWriter(filename_suffix=self.env_name)\n",
    "        optimizer = optim.Adam(self.policy_net.parameters(), lr=self.hp_learning_rate)\n",
    "\n",
    "        episode = 0\n",
    "        while True:\n",
    "            episode += 1\n",
    "            print(\"Start episode %d\" % episode)\n",
    "            self._reset()\n",
    "            done = False\n",
    "            ts_episode_started = time.time()\n",
    "\n",
    "            while(not done):\n",
    "                self.timesteps_overall += 1\n",
    "                done = self._play_step()\n",
    "                \n",
    "                if len(self.replay_memory) < self.hp_replay_memory_start_after:\n",
    "                    continue\n",
    "\n",
    "                #learn\n",
    "                optimizer.zero_grad()\n",
    "                batch = self.replay_memory.sample(self.hp_batch_size)\n",
    "                loss = self._calc_loss(batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if self.timesteps_overall % self.hp_target_update_after == 0:\n",
    "                    self._update_target_net()\n",
    "\n",
    "\n",
    "            speed = (self.timesteps_overall - self.timesteps_after_last_episode) / (time.time() - ts_episode_started)\n",
    "            self.timesteps_after_last_episode = self.timesteps_overall\n",
    "            \n",
    "            print(\"Episode %d completed, timesteps played: %d, return: %d, speed %f, epsilon %f\" \n",
    "                % (episode, self.timesteps_overall, self.last_obtained_return, speed, self._get_epsilon()))\n",
    "\n",
    "\n",
    "            self.train_obtained_returns.append(self.last_obtained_return)\n",
    "            self.train_avg_returns.append(np.mean(self.train_obtained_returns[-100:]))\n",
    "\n",
    "\n",
    "            writer.add_scalar('Training/AvgTotalReturn', self.train_avg_returns[-1], self.timesteps_overall)\n",
    "            writer.add_scalar('Training/ObtainedReturns', self.last_obtained_return, self.timesteps_overall)\n",
    "            writer.add_scalar('Parameter/Epsilon', self._get_epsilon(), self.timesteps_overall)\n",
    "            writer.add_scalar('Parameter/Speed', speed, self.timesteps_overall)\n",
    "            \n",
    "\n",
    "            print(\"Mean return of last 100 games: %f\" % self.train_avg_returns[-1])\n",
    "            \n",
    "            #evaluate every nr_of_frames_before_evaluation\n",
    "            if self.timesteps_overall >= self.nr_of_frames_before_evaluation * (self.eval_counter+1):\n",
    "                self._evaluate(writer)\n",
    "\n",
    "            if self.timesteps_overall >= self.nr_of_total_frames:\n",
    "                break\n",
    "\n",
    "        writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start episode 1\n",
      "Episode 1 completed, timesteps played: 875, return: -21, speed 1223.003971, epsilon 0.991250\n",
      "Mean return of last 100 games: -21.000000\n",
      "Start episode 2\n",
      "Episode 2 completed, timesteps played: 1829, return: -20, speed 1161.393375, epsilon 0.981710\n",
      "Mean return of last 100 games: -20.500000\n",
      "Start episode 3\n",
      "Episode 3 completed, timesteps played: 2705, return: -21, speed 1261.876312, epsilon 0.972950\n",
      "Mean return of last 100 games: -20.666667\n",
      "Start episode 4\n",
      "Episode 4 completed, timesteps played: 3508, return: -21, speed 1269.338555, epsilon 0.964920\n",
      "Mean return of last 100 games: -20.750000\n",
      "Start episode 5\n",
      "Episode 5 completed, timesteps played: 4313, return: -21, speed 1276.719416, epsilon 0.956870\n",
      "Mean return of last 100 games: -20.800000\n",
      "Start episode 6\n",
      "Episode 6 completed, timesteps played: 5148, return: -20, speed 1274.333765, epsilon 0.948520\n",
      "Mean return of last 100 games: -20.666667\n",
      "Start episode 7\n",
      "Episode 7 completed, timesteps played: 6323, return: -18, speed 1249.520734, epsilon 0.936770\n",
      "Mean return of last 100 games: -20.285714\n",
      "Start episode 8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fb423387440f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdqn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN_Agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-9e68c76017b7>\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimesteps_overall\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_play_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_memory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhp_replay_memory_start_after\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9e68c76017b7>\u001b[0m in \u001b[0;36m_play_step\u001b[0;34m(self, eval)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_select_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_obtained_return\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-68c8445c1248>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, ac)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mEpisodicLifeEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-68c8445c1248>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwas_real_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# check current lives, make loss of life terminal,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-68c8445c1248>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-68c8445c1248>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, ac)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mFireResetEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"ale.lives\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_get_obs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_get_image\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mgetScreenRGB2\u001b[0;34m(self, screen_data)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mscreen_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m480\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ctypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#env_name = 'PongNoFrameskip-v4'\n",
    "env_name = 'PongNoFrameskip-v4'\n",
    "env = make_atari(env_name, skip_noop=False, skip_maxskip=True)\n",
    "env = wrap_deepmind(env, pytorch_img=True, frame_stack=False)\n",
    "\n",
    "dqn = DQN_Agent(env, env_name)\n",
    "dqn.train_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic = np.array(dqn.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 84, 84)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f71957dea90>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "#%matplotlib inline\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "im = Image.fromarray(pic[3])\n",
    "imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f719573cc70>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "#%matplotlib inline\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "im = Image.fromarray(pic[0])\n",
    "imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.replay_memory.sample(10)[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
